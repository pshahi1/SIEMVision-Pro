#  Importing required libraries
from elasticsearch import Elasticsearch           # Elasticsearch client to connect and query the ELK stack
import pandas as pd                               # For handling and saving data in tabular (DataFrame) format
from datetime import datetime                     # For generating timestamped filenames
import os                                         # For file/directory management
import urllib3
## Disables SSL warnings when using self-signed certificates (local Elasticsearch).
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Step 1: Establishing connection to Elasticsearch
# Create a connection to the local Elasticsearch instance with basic authentication
es = Elasticsearch(
    hosts=["https://192.168.1.69:9200"],                     # Xubuntu SiemVision Pro's ip
    basic_auth=("elastic", "HlFTDthZxj1uSqBvDbEJ"), # Username and password (change these in production!)
    verify_certs=False                            # SSL verification disabled for local setup
    
)

# Step 2: Define the query for extracting Sysmon logs
query = {
    "size": 1000,  # Limit results to 5000 entries 
    "_source": [   # List of specific fields to extract from logs
        "event.code",
        "event.action",
        "host.name",
        "event.original",
        "agent.name",
        "log.level",
        "@timestamp",
        "event.created",
        "process.name",
        "winlog.event_data.ParentImage",
        "winlog.event_data.Image",
        "winlog.event_data.CommandLine",
        "winlog.channel"
    ],
    "query": {
        "bool": {
            "must": [  # Filter logs to only those generated by Microsoft-Windows-Sysmon
                {"match": {"event.provider": "Microsoft-Windows-Sysmon"}}
            ]
        }
    }
}

# Step 3: Execute the query and fetch the logs from Elasticsearch
print("üîç Querying Elasticsearch for Sysmon logs...")
try:
    response = es.search(index="winlogbeat-*", query=query["query"], _source=query["_source"], size=query["size"])
 # Query winlogbeat-* index pattern
    logs = [hit["_source"] for hit in response["hits"]["hits"]]  # Extract the _source field from each hit (log entry)
    df = pd.DataFrame(logs)  # Convert logs to a pandas DataFrame for easier manipulation
except Exception as e:
    # If connection or query fails, show error and exit
    print(f"‚ùå Elasticsearch query failed: {e}")
    exit(1)

# Step 4: Report number of logs retrieved
print(f"üìÑ Retrieved {len(df)} logs.")

# Step 5: Verify all expected fields are present
expected_cols = query["_source"]  # Columns we wanted in the output
missing = [col for col in expected_cols if col not in df.columns]  # Identify any missing fields
if missing:
    print(f"‚ö†Ô∏è Warning: Missing columns in data: {missing}")  # Log warning if any expected columns are missing

# Step 6: Prepare output directory to save the dataset
output_dir = "dataset/raw"  # Define output path relative to current script
os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't already exist

# Step 7: Generate a unique filename using current timestamp
timestamp = datetime.now().strftime("%Y%m%d_%H%M")  # Format: YYYYMMDD_HHMM
filename = f"sysmon_logs_{timestamp}.csv"  # Name the file with timestamp to track runs
output_path = os.path.join(output_dir, filename)  # Full path to the output file

# Step 8: Save the DataFrame to CSV file
df.to_csv(output_path, index=False)  # Save the extracted logs to CSV without index column
print(f"‚úÖ Dataset saved to: {output_path}")  # Confirm successful file save

# Append run metadata (filename, timestamp, log count) to a tracking file
metadata_path = os.path.join(output_dir, "csv_log.txt")  # Define the path to the metadata log file
readable_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Open the metadata log file in append mode so each run adds a new line
with open(metadata_path, "a") as f:
    f.write(f"{filename} | {readable_time} | {len(df)} logs\n")